{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/Users/mingu/Desktop/CODING/Kaggle Project 1/DATASET/train.csv\")\n",
    "test = pd.read_csv(\"/Users/mingu/Desktop/CODING/Kaggle Project 1/DATASET/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('Unnamed: 0',axis=1, inplace= True)\n",
    "test.drop('Unnamed: 0',axis=1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Species\n",
    "train.drop('Species', axis = 1, inplace = True)\n",
    "test.drop('Species', axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "### Farm.Name\n",
    "train.drop('Farm.Name', axis = 1, inplace = True)\n",
    "test.drop('Farm.Name', axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "#### Lot.Number\n",
    "import re\n",
    "import math\n",
    "\n",
    "def map_country_of_origin_preprocessing(value):\n",
    "    if pd.isna(value):\n",
    "        return -2\n",
    "    elif re.match(r'^\\d+/\\d+[A-Za-z]$', str(value)) or str(value).startswith('431')  or re.match(r'^\\d{3}/\\d{2}$', str(value)) or 'Lot' in str(value):\n",
    "        return 0\n",
    "    elif re.match(r'^3-\\d{2}-\\d{4}$', str(value)) or re.match(r'^\\d{2}-\\d{4}$', str(value)):\n",
    "        return 1\n",
    "    elif re.match(r'^11/\\d+/\\d+$', str(value)) or re.match(r'^11-\\d+-\\d+$', str(value)):\n",
    "        return 2\n",
    "    else:\n",
    "        return -1  # 예외 처리\n",
    "\n",
    "train['Mapped.Lot.Number.Country.of.Origin'] = train['Lot.Number'].apply(map_country_of_origin_preprocessing)\n",
    "test['Mapped.Lot.Number.Country.of.Origin'] = test['Lot.Number'].apply(map_country_of_origin_preprocessing)\n",
    "\n",
    "train.drop('Lot.Number', axis = 1, inplace = True)\n",
    "test.drop('Lot.Number', axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "### Altitude\n",
    "\n",
    "def to_meters(value):\n",
    "    if pd.isnull(value):\n",
    "        return np.nan\n",
    "    \n",
    "    if value.isdigit():\n",
    "        return float(value)  # 이미 숫자인 경우 그대로 반환합니다.\n",
    "    \n",
    "    if isinstance(value, str):\n",
    "        # 정규표현식을 사용하여 숫자를 추출합니다.\n",
    "        numbers = re.findall(r'\\d+\\.?\\d*', value)\n",
    "        value = value.replace('.', '')\n",
    "        \n",
    "        if len(numbers) == 1:\n",
    "            number = float(numbers[0])\n",
    "            # 단위에 따라 변환합니다.\n",
    "            if 'ft' in value.lower() or 'pies' in value.lower() or 'feet' in value.lower():\n",
    "                return number * 0.3048  # 피트(ft)를 미터(m)로 변환합니다.\n",
    "            elif 'mts' in value.lower() or 'metros' in value.lower() or 'msn' in value.lower() or 'msnm' in value.lower():\n",
    "                return number  # 미터(m)는 그대로 반환합니다.\n",
    "            elif 'km' in value.lower():\n",
    "                return number * 1000  # 킬로미터(km)를 미터(m)로 변환합니다.\n",
    "            elif 'miles' in value.lower():\n",
    "                return number * 1609.34  # 마일(miles)을 미터(m)로 변환합니다.\n",
    "            elif 'psn' in value.lower() or 'psnm' in value.lower():\n",
    "                return number * 1852\n",
    "            else:\n",
    "                return number\n",
    "        elif len(numbers) == 2:\n",
    "            # 숫자가 두 개인 경우에는 평균을 계산하여 반환합니다.\n",
    "            avg_number = (float(numbers[0]) + float(numbers[1])) / 2\n",
    "            if 'ft' in value.lower() or 'pies' in value.lower() or 'feet' in value.lower():\n",
    "                return avg_number * 0.3048  # 피트(ft)를 미터(m)로 변환합니다.\n",
    "            elif 'mts' in value.lower() or 'metros' in value.lower() or 'msn' in value.lower() or 'msnm' in value.lower():\n",
    "                return avg_number  # 미터(m)는 그대로 반환합니다.\n",
    "            elif 'km' in value.lower():\n",
    "                return avg_number * 1000  # 킬로미터(km)를 미터(m)로 변환합니다.\n",
    "            elif 'miles' in value.lower():\n",
    "                return avg_number * 1609.34  # 마일(miles)을 미터(m)로 변환합니다.\n",
    "            elif 'psn' in value.lower() or 'psnm' in value.lower():\n",
    "                return avg_number * 1852\n",
    "            else:\n",
    "                return avg_number\n",
    "    return np.nan\n",
    "\n",
    "train['converted_altitude'] = train['Altitude'].apply(to_meters)\n",
    "test['converted_altitude'] = test['Altitude'].apply(to_meters)\n",
    "\n",
    "mean_altitude = train['converted_altitude'].mean()\n",
    "train['converted_altitude'].fillna(mean_altitude, inplace=True)\n",
    "test['converted_altitude'].fillna(mean_altitude, inplace=True)\n",
    "\n",
    "train.drop('Altitude', axis = 1, inplace = True)\n",
    "test.drop('Altitude', axis = 1, inplace = True)\n",
    "\n",
    "### Bag.weight\n",
    "# Bag.Weight 열의 값에 따라 kg로 변환하는 함수 정의\n",
    "def convert_to_kg(value):\n",
    "    # 만약 값이 비어있으면 그대로 반환\n",
    "    if pd.isna(value):\n",
    "        return value\n",
    "    # 만약 값에 'kg'가 포함되어 있다면 이를 삭제한 후 반환\n",
    "    elif 'kg' in str(value):\n",
    "        return float(value.replace('kg', '').strip())\n",
    "    # 만약 값이 'lbs'로 끝나면 lbs를 kg로 변환하여 반환\n",
    "    elif str(value).endswith('lbs'):\n",
    "        weight_in_lbs = float(value.replace('lbs', '').strip())\n",
    "        weight_in_kg = weight_in_lbs * 0.453592  # 1 lbs = 0.453592 kg\n",
    "        return weight_in_kg\n",
    "    # 그 외의 경우는 단위가 없는 값으로 간주하여 kg로 반환\n",
    "    else:\n",
    "        return float(value)\n",
    "\n",
    "# Bag.Weight 열에 적용하여 모든 값을 kg로 변환\n",
    "train['Bag.Weight'] = train['Bag.Weight'].apply(convert_to_kg)\n",
    "test['Bag.Weight'] = test['Bag.Weight'].apply(convert_to_kg)\n",
    "\n",
    "### Harvest.Year\n",
    "# Harvest.Year 열의 값에서 연도를 추출하여 반환하는 함수 정의\n",
    "def extract_year(value):\n",
    "    # 값이 비어있으면 nan 반환\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    # 정규식을 사용하여 모든 연도 추출\n",
    "    matches = re.finditer(r'\\b(19|20)(\\d{2})\\b', str(value))\n",
    "    # 추출된 연도가 없으면 nan 반환\n",
    "    if not matches:\n",
    "        return np.nan\n",
    "    # 모든 매치의 연도를 리스트에 추가하여 반환\n",
    "    years = []\n",
    "    for match in matches:\n",
    "        years.append(int(match.group()))\n",
    "    return years\n",
    "\n",
    "# Harvest.Year 열에 적용하여 모든 값을 연도로 변환\n",
    "train['Harvest.Year'] = train['Harvest.Year'].apply(extract_year)\n",
    "test['Harvest.Year'] = test['Harvest.Year'].apply(extract_year)\n",
    "\n",
    "# 두 개 이상의 값이 있는 경우에는 두 값의 평균을 사용하는 함수 정의\n",
    "def handle_multiple_years(value):\n",
    "    # 만약 값이 nan이면 그대로 반환\n",
    "    # 만약 값이 리스트 형태가 아니라면 그대로 반환\n",
    "    if not isinstance(value, list):\n",
    "        return value\n",
    "    # 리스트의 길이가 1보다 작거나 같으면 그대로 반환\n",
    "    if len(value) <= 1:\n",
    "        return value[0] if len(value) == 1 else np.nan\n",
    "    # 리스트의 길이가 2 이상이면 두 값의 평균을 계산하여 반환\n",
    "    return np.mean(value)\n",
    "\n",
    "# Harvest.Year 열에 적용하여 모든 값을 연도로 변환\n",
    "train['Harvest.Year'] = train['Harvest.Year'].apply(handle_multiple_years)\n",
    "test['Harvest.Year'] = test['Harvest.Year'].apply(handle_multiple_years)\n",
    "\n",
    "max_harvest_year = train['Harvest.Year'].mode()[0]  # 최빈값 계산\n",
    "train['Harvest.Year'].fillna(max_harvest_year, inplace=True)  # nan 값을 최빈값으로 대체\n",
    "test['Harvest.Year'].fillna(max_harvest_year, inplace=True)\n",
    "\n",
    "train['Harvest.Year'] = train['Harvest.Year']\n",
    "test['Harvest.Year'] = test['Harvest.Year']\n",
    "\n",
    "### Grading.Date \n",
    "# Grading.Date 열의 값을 파싱하여 년, 월, 일로 나누는 함수 정의\n",
    "def parse_date(date_string):\n",
    "    try:\n",
    "        # 날짜 문자열을 datetime 객체로 변환\n",
    "        date = pd.to_datetime(date_string)\n",
    "        # 년, 월, 일 추출\n",
    "        year = date.year\n",
    "        month = date.month\n",
    "        day = date.day\n",
    "        return year, month, day\n",
    "    except:\n",
    "        return None, None, None\n",
    "\n",
    "# Grading.Date 열에서 년, 월, 일 추출하여 새로운 열에 추가\n",
    "train['Grading.Year'], train['Grading.Month'], train['Grading.Day'] = zip(*train['Grading.Date'].apply(parse_date))\n",
    "test['Grading.Year'], test['Grading.Month'], test['Grading.Day'] = zip(*test['Grading.Date'].apply(parse_date))\n",
    "\n",
    "# 월과 일에 대해 sin과 cos 함수를 적용하여 전처리하는 함수 정의\n",
    "def preprocess_seasonality(month, day):\n",
    "    # 월과 일을 0부터 1까지의 값으로 변환\n",
    "    normalized_month = (month - 1) / 12  # 1월부터 12월까지를 0부터 1까지의 값으로 변환\n",
    "    normalized_day = (day - 1) / 31  # 1일부터 31일까지를 0부터 1까지의 값으로 변환\n",
    "\n",
    "    # sin과 cos 함수를 적용하여 전처리\n",
    "    month_sin = -np.sin(2 * np.pi * normalized_month)\n",
    "    month_cos = -np.cos(2 * np.pi * normalized_month)\n",
    "    day_sin = -np.sin(2 * np.pi * normalized_day)\n",
    "    day_cos = -np.cos(2 * np.pi * normalized_day)\n",
    "    date_sin = -np.sin(2 * np.pi * (month+day/31)/12)\n",
    "    date_cos = -np.cos(2 * np.pi * (month+day/31)/12)\n",
    "    return month_sin, month_cos, day_sin, day_cos, date_sin, date_cos\n",
    "\n",
    "# Grading.Date 열에서 월과 일 추출하여 전처리 적용\n",
    "train['Grading.Month_Sin'], train['Grading.Month_Cos'], train['Grading.Day_Sin'], train['Grading.Day_Cos'], train['Grading.Date_Sin'], train['Grading.Date_Cos'] = zip(*train.apply(lambda x: preprocess_seasonality(x['Grading.Month'], x['Grading.Day']), axis=1))\n",
    "test['Grading.Month_Sin'], test['Grading.Month_Cos'], test['Grading.Day_Sin'], test['Grading.Day_Cos'], test['Grading.Date_Sin'], test['Grading.Date_Cos'] = zip(*test.apply(lambda x: preprocess_seasonality(x['Grading.Month'], x['Grading.Day']), axis=1))\n",
    "\n",
    "train.drop('Grading.Date', axis = 1, inplace = True)\n",
    "test.drop('Grading.Date', axis = 1, inplace = True)\n",
    "\n",
    "### Variety\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# LabelEncoder 객체 생성\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# train 데이터에서 Variety 열을 기반으로 라벨 인코딩 규칙을 학습하고 적용\n",
    "train['Variety'] = label_encoder.fit_transform(train['Variety'])\n",
    "# test 데이터에서 Variety 열을 라벨 인코딩 규칙을 적용 (train 데이터에서 학습된 규칙을 사용)\n",
    "test['Variety'] = label_encoder.transform(test['Variety'])\n",
    "\n",
    "### Expiration\n",
    "train['Expiration.Year'], train['Expiration.Month'], train['Expiration.Day'] = zip(*train['Expiration'].apply(parse_date))\n",
    "test['Expiration.Year'], test['Expiration.Month'], test['Expiration.Day'] = zip(*test['Expiration'].apply(parse_date))\n",
    "\n",
    "# Expiration 열에서 월과 일 추출하여 전처리 적용\n",
    "train['Expiration.Month_Sin'], train['Expiration.Month_Cos'], train['Expiration.Day_Sin'], train['Expiration.Day_Cos'], train['Expiration.Date_Sin'], train['Expiration.Date_Cos'] = zip(*train.apply(lambda x: preprocess_seasonality(x['Expiration.Month'], x['Expiration.Day']), axis=1))\n",
    "test['Expiration.Month_Sin'], test['Expiration.Month_Cos'], test['Expiration.Day_Sin'], test['Expiration.Day_Cos'], test['Expiration.Date_Sin'], test['Expiration.Date_Cos'] = zip(*test.apply(lambda x: preprocess_seasonality(x['Expiration.Month'], x['Expiration.Day']), axis=1))\n",
    "\n",
    "train.drop('Expiration', axis = 1, inplace = True)\n",
    "test.drop('Expiration', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing.Method\n",
      "Color\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# LabelEncoder 객체 생성\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# train 데이터프레임의 문자열 열에 대해 label encoding 수행\n",
    "for column in train.columns:\n",
    "    if train[column].dtype == 'object':\n",
    "        print(column)\n",
    "\n",
    "        # fit_transform()을 사용하여 train 데이터셋에 대해 label encoding 수행\n",
    "        train[column] = label_encoder.fit_transform(train[column])\n",
    "        \n",
    "        # test 데이터셋에 대해 transform()을 사용하여 train 데이터셋을 기반으로 동일한 label encoding 수행\n",
    "        # 만약 test 데이터셋에 대해 unseen label이 발생하면, 해당 값을 -1로 설정\n",
    "        test[column] = test[column].map(lambda s: label_encoder.transform([s])[0] if s in label_encoder.classes_ else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# StandardScaler 객체 생성\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scale_column = ['Number.of.Bags', 'Bag.Weight', 'Harvest.Year', 'Variety', 'Processing.Method']\n",
    "\n",
    "#for column in scale_column:\n",
    "for column in train.columns:\n",
    "    if column != 'Country.of.Origin':\n",
    "        # x_train 데이터에 대해 스케일링을 진행\n",
    "        train[column] = scaler.fit_transform(train[column].values.reshape(-1, 1))\n",
    "\n",
    "        # x_validation 데이터에 대해 스케일링을 진행 (훈련 데이터에서 추정된 변환 매개변수를 사용)\n",
    "        test[column] = scaler.transform(test[column].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fillna(-1, inplace = True)\n",
    "test.fillna(-1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number.of.Bags</th>\n",
       "      <th>Bag.Weight</th>\n",
       "      <th>Harvest.Year</th>\n",
       "      <th>Variety</th>\n",
       "      <th>Processing.Method</th>\n",
       "      <th>Aroma</th>\n",
       "      <th>Flavor</th>\n",
       "      <th>Aftertaste</th>\n",
       "      <th>Acidity</th>\n",
       "      <th>Body</th>\n",
       "      <th>...</th>\n",
       "      <th>Grading.Date_Cos</th>\n",
       "      <th>Expiration.Year</th>\n",
       "      <th>Expiration.Month</th>\n",
       "      <th>Expiration.Day</th>\n",
       "      <th>Expiration.Month_Sin</th>\n",
       "      <th>Expiration.Month_Cos</th>\n",
       "      <th>Expiration.Day_Sin</th>\n",
       "      <th>Expiration.Day_Cos</th>\n",
       "      <th>Expiration.Date_Sin</th>\n",
       "      <th>Expiration.Date_Cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.558407</td>\n",
       "      <td>-0.549787</td>\n",
       "      <td>-0.743565</td>\n",
       "      <td>-1.048871</td>\n",
       "      <td>0.443524</td>\n",
       "      <td>-1.218672</td>\n",
       "      <td>-4.374557</td>\n",
       "      <td>-3.558456</td>\n",
       "      <td>-0.920381</td>\n",
       "      <td>-0.608264</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.263458</td>\n",
       "      <td>-0.751825</td>\n",
       "      <td>-1.719060</td>\n",
       "      <td>-0.441136</td>\n",
       "      <td>-0.031414</td>\n",
       "      <td>-1.622870</td>\n",
       "      <td>-1.078131</td>\n",
       "      <td>0.891512</td>\n",
       "      <td>-1.083114</td>\n",
       "      <td>-1.279230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.758483</td>\n",
       "      <td>0.453644</td>\n",
       "      <td>-0.743565</td>\n",
       "      <td>-0.572481</td>\n",
       "      <td>1.068734</td>\n",
       "      <td>-0.696480</td>\n",
       "      <td>-0.502131</td>\n",
       "      <td>-0.023442</td>\n",
       "      <td>0.227939</td>\n",
       "      <td>-0.912982</td>\n",
       "      <td>...</td>\n",
       "      <td>0.649597</td>\n",
       "      <td>-1.285585</td>\n",
       "      <td>0.201341</td>\n",
       "      <td>1.357222</td>\n",
       "      <td>-0.031414</td>\n",
       "      <td>1.172992</td>\n",
       "      <td>1.019472</td>\n",
       "      <td>-0.990739</td>\n",
       "      <td>0.970983</td>\n",
       "      <td>0.671765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.242277</td>\n",
       "      <td>0.439102</td>\n",
       "      <td>2.006026</td>\n",
       "      <td>-1.048871</td>\n",
       "      <td>0.443524</td>\n",
       "      <td>-0.141650</td>\n",
       "      <td>-0.223316</td>\n",
       "      <td>0.250827</td>\n",
       "      <td>0.819498</td>\n",
       "      <td>0.039262</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.353847</td>\n",
       "      <td>1.916972</td>\n",
       "      <td>0.841475</td>\n",
       "      <td>-0.890725</td>\n",
       "      <td>1.242790</td>\n",
       "      <td>0.474027</td>\n",
       "      <td>-1.351745</td>\n",
       "      <td>-0.213086</td>\n",
       "      <td>1.177659</td>\n",
       "      <td>-0.352849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.998575</td>\n",
       "      <td>-0.535244</td>\n",
       "      <td>0.356272</td>\n",
       "      <td>-0.810676</td>\n",
       "      <td>-2.057312</td>\n",
       "      <td>0.413179</td>\n",
       "      <td>0.551169</td>\n",
       "      <td>1.012683</td>\n",
       "      <td>-0.050442</td>\n",
       "      <td>-0.265456</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.602792</td>\n",
       "      <td>0.849453</td>\n",
       "      <td>-1.398993</td>\n",
       "      <td>-0.328738</td>\n",
       "      <td>-0.767077</td>\n",
       "      <td>-1.435583</td>\n",
       "      <td>-0.885192</td>\n",
       "      <td>1.103672</td>\n",
       "      <td>-1.479327</td>\n",
       "      <td>-0.601700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.434350</td>\n",
       "      <td>-0.531348</td>\n",
       "      <td>-0.193646</td>\n",
       "      <td>1.571270</td>\n",
       "      <td>0.443524</td>\n",
       "      <td>-0.141650</td>\n",
       "      <td>-0.223316</td>\n",
       "      <td>0.250827</td>\n",
       "      <td>-0.328822</td>\n",
       "      <td>0.686788</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.454523</td>\n",
       "      <td>0.315694</td>\n",
       "      <td>0.841475</td>\n",
       "      <td>-0.441136</td>\n",
       "      <td>1.242790</td>\n",
       "      <td>0.474027</td>\n",
       "      <td>-1.078131</td>\n",
       "      <td>0.891512</td>\n",
       "      <td>1.162148</td>\n",
       "      <td>-0.453488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>0.758483</td>\n",
       "      <td>-0.535244</td>\n",
       "      <td>1.181149</td>\n",
       "      <td>0.380297</td>\n",
       "      <td>0.443524</td>\n",
       "      <td>0.935372</td>\n",
       "      <td>0.799004</td>\n",
       "      <td>1.500271</td>\n",
       "      <td>0.227939</td>\n",
       "      <td>0.686788</td>\n",
       "      <td>...</td>\n",
       "      <td>1.356392</td>\n",
       "      <td>0.849453</td>\n",
       "      <td>-0.118726</td>\n",
       "      <td>-1.340314</td>\n",
       "      <td>-0.767077</td>\n",
       "      <td>0.985705</td>\n",
       "      <td>-0.774324</td>\n",
       "      <td>-1.181407</td>\n",
       "      <td>-0.083711</td>\n",
       "      <td>1.358683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>0.158255</td>\n",
       "      <td>0.439102</td>\n",
       "      <td>2.006026</td>\n",
       "      <td>0.142102</td>\n",
       "      <td>0.443524</td>\n",
       "      <td>0.935372</td>\n",
       "      <td>1.046839</td>\n",
       "      <td>0.738415</td>\n",
       "      <td>1.411057</td>\n",
       "      <td>-0.265456</td>\n",
       "      <td>...</td>\n",
       "      <td>1.119247</td>\n",
       "      <td>1.916972</td>\n",
       "      <td>0.201341</td>\n",
       "      <td>-1.452712</td>\n",
       "      <td>-0.031414</td>\n",
       "      <td>1.172992</td>\n",
       "      <td>-0.529471</td>\n",
       "      <td>-1.323464</td>\n",
       "      <td>0.566132</td>\n",
       "      <td>1.119689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>0.558407</td>\n",
       "      <td>-0.549787</td>\n",
       "      <td>-0.743565</td>\n",
       "      <td>-1.048871</td>\n",
       "      <td>0.443524</td>\n",
       "      <td>-0.141650</td>\n",
       "      <td>0.551169</td>\n",
       "      <td>0.494621</td>\n",
       "      <td>-0.050442</td>\n",
       "      <td>0.343980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997035</td>\n",
       "      <td>-0.751825</td>\n",
       "      <td>0.201341</td>\n",
       "      <td>-0.553533</td>\n",
       "      <td>-0.031414</td>\n",
       "      <td>1.172992</td>\n",
       "      <td>-1.226263</td>\n",
       "      <td>0.643098</td>\n",
       "      <td>0.713870</td>\n",
       "      <td>0.997523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>0.558407</td>\n",
       "      <td>0.453644</td>\n",
       "      <td>-0.743565</td>\n",
       "      <td>1.571270</td>\n",
       "      <td>1.068734</td>\n",
       "      <td>0.119446</td>\n",
       "      <td>0.551169</td>\n",
       "      <td>0.250827</td>\n",
       "      <td>-1.198762</td>\n",
       "      <td>-1.217700</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.699150</td>\n",
       "      <td>-1.285585</td>\n",
       "      <td>-1.398993</td>\n",
       "      <td>-0.778328</td>\n",
       "      <td>-0.767077</td>\n",
       "      <td>-1.435583</td>\n",
       "      <td>-1.365929</td>\n",
       "      <td>0.079257</td>\n",
       "      <td>-1.448746</td>\n",
       "      <td>-0.698022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>-0.337934</td>\n",
       "      <td>0.453644</td>\n",
       "      <td>0.356272</td>\n",
       "      <td>-0.572481</td>\n",
       "      <td>0.443524</td>\n",
       "      <td>0.413179</td>\n",
       "      <td>0.551169</td>\n",
       "      <td>0.494621</td>\n",
       "      <td>-0.050442</td>\n",
       "      <td>0.686788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.775304</td>\n",
       "      <td>0.315694</td>\n",
       "      <td>-0.758859</td>\n",
       "      <td>-0.890725</td>\n",
       "      <td>-1.502739</td>\n",
       "      <td>-0.224939</td>\n",
       "      <td>-1.351745</td>\n",
       "      <td>-0.213086</td>\n",
       "      <td>-1.256166</td>\n",
       "      <td>0.775876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>585 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Number.of.Bags  Bag.Weight  Harvest.Year   Variety  Processing.Method  \\\n",
       "0          0.558407   -0.549787     -0.743565 -1.048871           0.443524   \n",
       "1          0.758483    0.453644     -0.743565 -0.572481           1.068734   \n",
       "2         -1.242277    0.439102      2.006026 -1.048871           0.443524   \n",
       "3          0.998575   -0.535244      0.356272 -0.810676          -2.057312   \n",
       "4         -1.434350   -0.531348     -0.193646  1.571270           0.443524   \n",
       "..              ...         ...           ...       ...                ...   \n",
       "580        0.758483   -0.535244      1.181149  0.380297           0.443524   \n",
       "581        0.158255    0.439102      2.006026  0.142102           0.443524   \n",
       "582        0.558407   -0.549787     -0.743565 -1.048871           0.443524   \n",
       "583        0.558407    0.453644     -0.743565  1.571270           1.068734   \n",
       "584       -0.337934    0.453644      0.356272 -0.572481           0.443524   \n",
       "\n",
       "        Aroma    Flavor  Aftertaste   Acidity      Body  ...  \\\n",
       "0   -1.218672 -4.374557   -3.558456 -0.920381 -0.608264  ...   \n",
       "1   -0.696480 -0.502131   -0.023442  0.227939 -0.912982  ...   \n",
       "2   -0.141650 -0.223316    0.250827  0.819498  0.039262  ...   \n",
       "3    0.413179  0.551169    1.012683 -0.050442 -0.265456  ...   \n",
       "4   -0.141650 -0.223316    0.250827 -0.328822  0.686788  ...   \n",
       "..        ...       ...         ...       ...       ...  ...   \n",
       "580  0.935372  0.799004    1.500271  0.227939  0.686788  ...   \n",
       "581  0.935372  1.046839    0.738415  1.411057 -0.265456  ...   \n",
       "582 -0.141650  0.551169    0.494621 -0.050442  0.343980  ...   \n",
       "583  0.119446  0.551169    0.250827 -1.198762 -1.217700  ...   \n",
       "584  0.413179  0.551169    0.494621 -0.050442  0.686788  ...   \n",
       "\n",
       "     Grading.Date_Cos  Expiration.Year  Expiration.Month  Expiration.Day  \\\n",
       "0           -1.263458        -0.751825         -1.719060       -0.441136   \n",
       "1            0.649597        -1.285585          0.201341        1.357222   \n",
       "2           -0.353847         1.916972          0.841475       -0.890725   \n",
       "3           -0.602792         0.849453         -1.398993       -0.328738   \n",
       "4           -0.454523         0.315694          0.841475       -0.441136   \n",
       "..                ...              ...               ...             ...   \n",
       "580          1.356392         0.849453         -0.118726       -1.340314   \n",
       "581          1.119247         1.916972          0.201341       -1.452712   \n",
       "582          0.997035        -0.751825          0.201341       -0.553533   \n",
       "583         -0.699150        -1.285585         -1.398993       -0.778328   \n",
       "584          0.775304         0.315694         -0.758859       -0.890725   \n",
       "\n",
       "     Expiration.Month_Sin  Expiration.Month_Cos  Expiration.Day_Sin  \\\n",
       "0               -0.031414             -1.622870           -1.078131   \n",
       "1               -0.031414              1.172992            1.019472   \n",
       "2                1.242790              0.474027           -1.351745   \n",
       "3               -0.767077             -1.435583           -0.885192   \n",
       "4                1.242790              0.474027           -1.078131   \n",
       "..                    ...                   ...                 ...   \n",
       "580             -0.767077              0.985705           -0.774324   \n",
       "581             -0.031414              1.172992           -0.529471   \n",
       "582             -0.031414              1.172992           -1.226263   \n",
       "583             -0.767077             -1.435583           -1.365929   \n",
       "584             -1.502739             -0.224939           -1.351745   \n",
       "\n",
       "     Expiration.Day_Cos  Expiration.Date_Sin  Expiration.Date_Cos  \n",
       "0              0.891512            -1.083114            -1.279230  \n",
       "1             -0.990739             0.970983             0.671765  \n",
       "2             -0.213086             1.177659            -0.352849  \n",
       "3              1.103672            -1.479327            -0.601700  \n",
       "4              0.891512             1.162148            -0.453488  \n",
       "..                  ...                  ...                  ...  \n",
       "580           -1.181407            -0.083711             1.358683  \n",
       "581           -1.323464             0.566132             1.119689  \n",
       "582            0.643098             0.713870             0.997523  \n",
       "583            0.079257            -1.448746            -0.698022  \n",
       "584           -0.213086            -1.256166             0.775876  \n",
       "\n",
       "[585 rows x 42 columns]"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Number.of.Bags                        -0.387300\n",
       "Bag.Weight                            -0.234214\n",
       "Harvest.Year                          -0.312995\n",
       "Variety                                0.121877\n",
       "Processing.Method                      0.399663\n",
       "Aroma                                 -0.170782\n",
       "Flavor                                -0.261109\n",
       "Aftertaste                            -0.310908\n",
       "Acidity                               -0.114522\n",
       "Body                                  -0.290722\n",
       "Balance                               -0.316077\n",
       "Uniformity                            -0.100379\n",
       "Clean.Cup                             -0.039066\n",
       "Sweetness                              0.000975\n",
       "Cupper.Points                         -0.302856\n",
       "Total.Cup.Points                      -0.258803\n",
       "Moisture                               0.331794\n",
       "Category.One.Defects                   0.132010\n",
       "Quakers                               -0.138671\n",
       "Color                                  0.185238\n",
       "Category.Two.Defects                   0.250217\n",
       "Country.of.Origin                      1.000000\n",
       "Mapped.Lot.Number.Country.of.Origin   -0.130755\n",
       "converted_altitude                     0.032977\n",
       "Grading.Year                          -0.324602\n",
       "Grading.Month                         -0.018535\n",
       "Grading.Day                           -0.014816\n",
       "Grading.Month_Sin                     -0.090866\n",
       "Grading.Month_Cos                      0.459364\n",
       "Grading.Day_Sin                       -0.099292\n",
       "Grading.Day_Cos                       -0.244634\n",
       "Grading.Date_Sin                       0.237552\n",
       "Grading.Date_Cos                       0.415500\n",
       "Expiration.Year                       -0.324602\n",
       "Expiration.Month                      -0.018078\n",
       "Expiration.Day                        -0.005313\n",
       "Expiration.Month_Sin                  -0.091510\n",
       "Expiration.Month_Cos                   0.458749\n",
       "Expiration.Day_Sin                    -0.100968\n",
       "Expiration.Day_Cos                    -0.244744\n",
       "Expiration.Date_Sin                    0.235917\n",
       "Expiration.Date_Cos                    0.415576\n",
       "Name: Country.of.Origin, dtype: float64"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.corr()['Country.of.Origin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('Country.of.Origin', axis = 1)\n",
    "y = train['Country.of.Origin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_iters': 100, 'lr': 1e-05, 'lambda_param': 1e-05} : 0.6273504273504273\n",
      "{'n_iters': 100, 'lr': 1e-05, 'lambda_param': 0.0001} : 0.6273504273504273\n",
      "{'n_iters': 100, 'lr': 1e-05, 'lambda_param': 0.001} : 0.6273504273504273\n",
      "{'n_iters': 100, 'lr': 1e-05, 'lambda_param': 0.01} : 0.6273504273504273\n",
      "{'n_iters': 100, 'lr': 0.0001, 'lambda_param': 1e-05} : 0.6991452991452991\n",
      "{'n_iters': 100, 'lr': 0.0001, 'lambda_param': 0.0001} : 0.6991452991452991\n",
      "{'n_iters': 100, 'lr': 0.0001, 'lambda_param': 0.001} : 0.6991452991452991\n",
      "{'n_iters': 100, 'lr': 0.0001, 'lambda_param': 0.01} : 0.6923076923076923\n",
      "{'n_iters': 100, 'lr': 0.001, 'lambda_param': 1e-05} : 0.6683760683760684\n",
      "{'n_iters': 100, 'lr': 0.001, 'lambda_param': 0.0001} : 0.6683760683760684\n",
      "{'n_iters': 100, 'lr': 0.001, 'lambda_param': 0.001} : 0.6666666666666667\n",
      "{'n_iters': 100, 'lr': 0.001, 'lambda_param': 0.01} : 0.6341880341880343\n",
      "{'n_iters': 100, 'lr': 0.01, 'lambda_param': 1e-05} : 0.6735042735042736\n",
      "{'n_iters': 100, 'lr': 0.01, 'lambda_param': 0.0001} : 0.6632478632478633\n",
      "{'n_iters': 100, 'lr': 0.01, 'lambda_param': 0.001} : 0.6307692307692309\n",
      "{'n_iters': 100, 'lr': 0.01, 'lambda_param': 0.01} : 0.6188034188034188\n",
      "{'n_iters': 200, 'lr': 1e-05, 'lambda_param': 1e-05} : 0.6307692307692307\n",
      "{'n_iters': 200, 'lr': 1e-05, 'lambda_param': 0.0001} : 0.6307692307692307\n",
      "{'n_iters': 200, 'lr': 1e-05, 'lambda_param': 0.001} : 0.6307692307692307\n",
      "{'n_iters': 200, 'lr': 1e-05, 'lambda_param': 0.01} : 0.6307692307692307\n",
      "{'n_iters': 200, 'lr': 0.0001, 'lambda_param': 1e-05} : 0.6735042735042736\n",
      "{'n_iters': 200, 'lr': 0.0001, 'lambda_param': 0.0001} : 0.6735042735042736\n",
      "{'n_iters': 200, 'lr': 0.0001, 'lambda_param': 0.001} : 0.6735042735042736\n",
      "{'n_iters': 200, 'lr': 0.0001, 'lambda_param': 0.01} : 0.6666666666666667\n",
      "{'n_iters': 200, 'lr': 0.001, 'lambda_param': 1e-05} : 0.6700854700854701\n",
      "{'n_iters': 200, 'lr': 0.001, 'lambda_param': 0.0001} : 0.6683760683760684\n",
      "{'n_iters': 200, 'lr': 0.001, 'lambda_param': 0.001} : 0.6512820512820513\n",
      "{'n_iters': 200, 'lr': 0.001, 'lambda_param': 0.01} : 0.6273504273504275\n",
      "{'n_iters': 200, 'lr': 0.01, 'lambda_param': 1e-05} : 0.6717948717948719\n",
      "{'n_iters': 200, 'lr': 0.01, 'lambda_param': 0.0001} : 0.6512820512820513\n",
      "{'n_iters': 200, 'lr': 0.01, 'lambda_param': 0.001} : 0.6290598290598292\n",
      "{'n_iters': 200, 'lr': 0.01, 'lambda_param': 0.01} : 0.6188034188034188\n",
      "{'n_iters': 300, 'lr': 1e-05, 'lambda_param': 1e-05} : 0.6495726495726496\n",
      "{'n_iters': 300, 'lr': 1e-05, 'lambda_param': 0.0001} : 0.6495726495726496\n",
      "{'n_iters': 300, 'lr': 1e-05, 'lambda_param': 0.001} : 0.6495726495726496\n",
      "{'n_iters': 300, 'lr': 1e-05, 'lambda_param': 0.01} : 0.6512820512820513\n",
      "{'n_iters': 300, 'lr': 0.0001, 'lambda_param': 1e-05} : 0.6683760683760684\n",
      "{'n_iters': 300, 'lr': 0.0001, 'lambda_param': 0.0001} : 0.6700854700854701\n",
      "{'n_iters': 300, 'lr': 0.0001, 'lambda_param': 0.001} : 0.6666666666666667\n",
      "{'n_iters': 300, 'lr': 0.0001, 'lambda_param': 0.01} : 0.6478632478632479\n",
      "{'n_iters': 300, 'lr': 0.001, 'lambda_param': 1e-05} : 0.6735042735042736\n",
      "{'n_iters': 300, 'lr': 0.001, 'lambda_param': 0.0001} : 0.6717948717948719\n",
      "{'n_iters': 300, 'lr': 0.001, 'lambda_param': 0.001} : 0.6393162393162394\n",
      "{'n_iters': 300, 'lr': 0.001, 'lambda_param': 0.01} : 0.6222222222222222\n",
      "{'n_iters': 300, 'lr': 0.01, 'lambda_param': 1e-05} : 0.6717948717948719\n",
      "{'n_iters': 300, 'lr': 0.01, 'lambda_param': 0.0001} : 0.6410256410256411\n",
      "{'n_iters': 300, 'lr': 0.01, 'lambda_param': 0.001} : 0.6222222222222222\n",
      "{'n_iters': 300, 'lr': 0.01, 'lambda_param': 0.01} : 0.6188034188034188\n",
      "{'n_iters': 400, 'lr': 1e-05, 'lambda_param': 1e-05} : 0.6615384615384616\n",
      "{'n_iters': 400, 'lr': 1e-05, 'lambda_param': 0.0001} : 0.6615384615384616\n",
      "{'n_iters': 400, 'lr': 1e-05, 'lambda_param': 0.001} : 0.6615384615384616\n",
      "{'n_iters': 400, 'lr': 1e-05, 'lambda_param': 0.01} : 0.6615384615384616\n",
      "{'n_iters': 400, 'lr': 0.0001, 'lambda_param': 1e-05} : 0.6632478632478633\n",
      "{'n_iters': 400, 'lr': 0.0001, 'lambda_param': 0.0001} : 0.6632478632478633\n",
      "{'n_iters': 400, 'lr': 0.0001, 'lambda_param': 0.001} : 0.6632478632478633\n",
      "{'n_iters': 400, 'lr': 0.0001, 'lambda_param': 0.01} : 0.6444444444444445\n",
      "{'n_iters': 400, 'lr': 0.001, 'lambda_param': 1e-05} : 0.6735042735042736\n",
      "{'n_iters': 400, 'lr': 0.001, 'lambda_param': 0.0001} : 0.6700854700854701\n",
      "{'n_iters': 400, 'lr': 0.001, 'lambda_param': 0.001} : 0.6376068376068377\n",
      "{'n_iters': 400, 'lr': 0.001, 'lambda_param': 0.01} : 0.6256410256410256\n",
      "{'n_iters': 400, 'lr': 0.01, 'lambda_param': 1e-05} : 0.6700854700854701\n",
      "{'n_iters': 400, 'lr': 0.01, 'lambda_param': 0.0001} : 0.6376068376068377\n",
      "{'n_iters': 400, 'lr': 0.01, 'lambda_param': 0.001} : 0.6273504273504273\n",
      "{'n_iters': 400, 'lr': 0.01, 'lambda_param': 0.01} : 0.6188034188034188\n",
      "{'n_iters': 500, 'lr': 1e-05, 'lambda_param': 1e-05} : 0.6735042735042736\n",
      "{'n_iters': 500, 'lr': 1e-05, 'lambda_param': 0.0001} : 0.6735042735042736\n",
      "{'n_iters': 500, 'lr': 1e-05, 'lambda_param': 0.001} : 0.6735042735042736\n",
      "{'n_iters': 500, 'lr': 1e-05, 'lambda_param': 0.01} : 0.6752136752136753\n",
      "{'n_iters': 500, 'lr': 0.0001, 'lambda_param': 1e-05} : 0.6683760683760684\n",
      "{'n_iters': 500, 'lr': 0.0001, 'lambda_param': 0.0001} : 0.6683760683760684\n",
      "{'n_iters': 500, 'lr': 0.0001, 'lambda_param': 0.001} : 0.6632478632478633\n",
      "{'n_iters': 500, 'lr': 0.0001, 'lambda_param': 0.01} : 0.6393162393162393\n",
      "{'n_iters': 500, 'lr': 0.001, 'lambda_param': 1e-05} : 0.6735042735042736\n",
      "{'n_iters': 500, 'lr': 0.001, 'lambda_param': 0.0001} : 0.6683760683760684\n",
      "{'n_iters': 500, 'lr': 0.001, 'lambda_param': 0.001} : 0.6341880341880342\n",
      "{'n_iters': 500, 'lr': 0.001, 'lambda_param': 0.01} : 0.6205128205128205\n",
      "{'n_iters': 500, 'lr': 0.01, 'lambda_param': 1e-05} : 0.6666666666666667\n",
      "{'n_iters': 500, 'lr': 0.01, 'lambda_param': 0.0001} : 0.6341880341880342\n",
      "{'n_iters': 500, 'lr': 0.01, 'lambda_param': 0.001} : 0.6205128205128205\n",
      "{'n_iters': 500, 'lr': 0.01, 'lambda_param': 0.01} : 0.6188034188034188\n",
      "{'n_iters': 600, 'lr': 1e-05, 'lambda_param': 1e-05} : 0.6803418803418804\n",
      "{'n_iters': 600, 'lr': 1e-05, 'lambda_param': 0.0001} : 0.6803418803418804\n",
      "{'n_iters': 600, 'lr': 1e-05, 'lambda_param': 0.001} : 0.6803418803418804\n",
      "{'n_iters': 600, 'lr': 1e-05, 'lambda_param': 0.01} : 0.6837606837606838\n",
      "{'n_iters': 600, 'lr': 0.0001, 'lambda_param': 1e-05} : 0.6666666666666667\n",
      "{'n_iters': 600, 'lr': 0.0001, 'lambda_param': 0.0001} : 0.6666666666666667\n",
      "{'n_iters': 600, 'lr': 0.0001, 'lambda_param': 0.001} : 0.6632478632478633\n",
      "{'n_iters': 600, 'lr': 0.0001, 'lambda_param': 0.01} : 0.6393162393162393\n",
      "{'n_iters': 600, 'lr': 0.001, 'lambda_param': 1e-05} : 0.6735042735042736\n",
      "{'n_iters': 600, 'lr': 0.001, 'lambda_param': 0.0001} : 0.6666666666666667\n",
      "{'n_iters': 600, 'lr': 0.001, 'lambda_param': 0.001} : 0.6307692307692307\n",
      "{'n_iters': 600, 'lr': 0.001, 'lambda_param': 0.01} : 0.6222222222222222\n",
      "{'n_iters': 600, 'lr': 0.01, 'lambda_param': 1e-05} : 0.6666666666666667\n",
      "{'n_iters': 600, 'lr': 0.01, 'lambda_param': 0.0001} : 0.6307692307692307\n",
      "{'n_iters': 600, 'lr': 0.01, 'lambda_param': 0.001} : 0.6205128205128205\n",
      "{'n_iters': 600, 'lr': 0.01, 'lambda_param': 0.01} : 0.6188034188034188\n",
      "{'n_iters': 700, 'lr': 1e-05, 'lambda_param': 1e-05} : 0.6888888888888889\n",
      "{'n_iters': 700, 'lr': 1e-05, 'lambda_param': 0.0001} : 0.6888888888888889\n",
      "{'n_iters': 700, 'lr': 1e-05, 'lambda_param': 0.001} : 0.6888888888888889\n",
      "{'n_iters': 700, 'lr': 1e-05, 'lambda_param': 0.01} : 0.694017094017094\n",
      "{'n_iters': 700, 'lr': 0.0001, 'lambda_param': 1e-05} : 0.6666666666666667\n",
      "{'n_iters': 700, 'lr': 0.0001, 'lambda_param': 0.0001} : 0.6683760683760684\n",
      "{'n_iters': 700, 'lr': 0.0001, 'lambda_param': 0.001} : 0.6615384615384616\n",
      "{'n_iters': 700, 'lr': 0.0001, 'lambda_param': 0.01} : 0.635897435897436\n",
      "{'n_iters': 700, 'lr': 0.001, 'lambda_param': 1e-05} : 0.6735042735042736\n",
      "{'n_iters': 700, 'lr': 0.001, 'lambda_param': 0.0001} : 0.6666666666666667\n",
      "{'n_iters': 700, 'lr': 0.001, 'lambda_param': 0.001} : 0.635897435897436\n",
      "{'n_iters': 700, 'lr': 0.001, 'lambda_param': 0.01} : 0.6205128205128205\n",
      "{'n_iters': 700, 'lr': 0.01, 'lambda_param': 1e-05} : 0.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from SVM import MulticlassSVMClassifier\n",
    "import numpy as np\n",
    "\n",
    "def multiclass_svm_grid_search(X, y, X_test, param_grid, n_splits=5, random_seed=3):\n",
    "    # Set the random seed for reproducibility\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Initialize variables to store the best score and best parameters\n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "    best_predict = []\n",
    "    \n",
    "    # Perform k-fold cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "    for n_iters in param_grid['n_iters']:\n",
    "        for lr in param_grid['lr']:\n",
    "            for lambda_param in param_grid['lambda_param']:\n",
    "                # Initialize SVM classifier with current hyperparameters\n",
    "                svm = MulticlassSVMClassifier(n_iters = n_iters, lr = lr, lambda_param = lambda_param)\n",
    "                \n",
    "                # Initialize variables to store the scores for this combination of hyperparameters\n",
    "                scores = []\n",
    "                predict = []\n",
    "                for train_idx, val_idx in skf.split(X, y):\n",
    "                    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "                    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "                    smote = SMOTE(random_state=random_seed)\n",
    "\n",
    "                    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "                    X_train, y_train = X_train.values, y_train.values\n",
    "                    X_val, y_val = X_val.values, y_val.values\n",
    "                    \n",
    "                    # Train the model\n",
    "                    svm.fit(X_train, y_train)\n",
    "\n",
    "                    # Evaluate the model\n",
    "                    predictions = svm.predict(X_val)\n",
    "                    accuracy = svm.get_accuracy(y_val, predictions)\n",
    "                    scores.append(accuracy)\n",
    "\n",
    "                    #test prediction\n",
    "                    predict.append(svm.predict(X_test))\n",
    "                \n",
    "                # Calculate the average score for this combination of hyperparameters\n",
    "                avg_score = np.mean(scores)\n",
    "                \n",
    "                params = {'n_iters': n_iters, 'lr': lr, 'lambda_param': lambda_param}\n",
    "                print(params,':',avg_score)\n",
    "                \n",
    "                # Check if the current combination of hyperparameters yields a better score\n",
    "                if avg_score > best_score:\n",
    "                    best_score = avg_score\n",
    "                    best_params = {'n_iters': n_iters, 'lr': lr, 'lambda_param': lambda_param}\n",
    "                    best_predict = predict[:]\n",
    "    \n",
    "    return best_params, best_score, best_predict\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_iters': [i* 100 for i in range(1,11)],\n",
    "    'lr': [0.00001, 0.0001 ,0.001, 0.01 ],\n",
    "    'lambda_param': [0.00001, 0.0001, 0.001, 0.01],\n",
    "}\n",
    "\n",
    "# Assuming X contains your features and y contains your target variable\n",
    "\n",
    "# Perform grid search\n",
    "best_params, best_score, best_predict = multiclass_svm_grid_search(X, y, test.values,  param_grid)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "print(\"Best Predict:\", best_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test = np.array(best_predict)\n",
    "final_predictions = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=0, arr=prediction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 2, 0, 3, 2, 0, 3, 3, 3, 0, 3, 0, 3, 0, 2, 0, 1, 1, 3, 2, 3,\n",
       "       2, 0, 0, 3, 2, 3, 3, 3, 3, 1, 2, 3, 3, 3, 3, 2, 0, 1, 0, 3, 0, 2,\n",
       "       0, 0, 2, 0, 1, 2, 0, 0, 3, 1, 0, 1, 0, 0, 2, 2, 0, 2, 0, 2, 0, 0,\n",
       "       3, 3, 0, 2, 2, 1, 0, 0, 0, 2, 2, 3, 0, 2, 3, 1, 3, 3, 0, 3, 1, 0,\n",
       "       0, 2, 3, 3, 3, 2, 3, 2, 0, 0, 1, 1, 2, 1, 3, 0, 1, 0, 2, 1, 0, 0,\n",
       "       0, 2, 0, 2, 0, 2, 0, 0, 2, 3, 0, 1, 1, 1, 3, 3, 2, 2, 1, 2, 3, 0,\n",
       "       3, 3, 3, 0, 3, 0, 0, 2, 1, 1, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/Users/mingu/Desktop/CODING/Kaggle Project 1/DATASET/train.csv\")\n",
    "test = pd.read_csv(\"/Users/mingu/Desktop/CODING/Kaggle Project 1/DATASET/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Country.of.Origin'] = final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "### Altitude <- 1,2 혼동이 되는 경우가 있기 때문에 후처리 시 앞으로 배정\n",
    "def map_country_based_on_altitude(country, altitude_text):\n",
    "    if isinstance(altitude_text, str):  # 문자열인지 확인\n",
    "        if 'msn' in altitude_text:\n",
    "            if country not in [1, 2]:\n",
    "                return 1\n",
    "        elif 'psn' in altitude_text or 'ft' in altitude_text:\n",
    "            return 2\n",
    "    return country\n",
    "\n",
    "test['Country.of.Origin'] = test.apply(lambda row: map_country_based_on_altitude(row['Country.of.Origin'], row['Altitude']), axis=1)\n",
    "\n",
    "###Farm.Name\n",
    "\n",
    "farm_country_mapping = train[train['Farm.Name'].isin(['santa maria', 'several', 'various']) == False].groupby('Farm.Name')['Country.of.Origin'].first().to_dict()\n",
    "\n",
    "# 후처리 함수 정의\n",
    "def postprocess_country(df, farm_country_mapping):\n",
    "    for index, row in df.iterrows():\n",
    "        farm_name = row['Farm.Name']\n",
    "        if farm_name in farm_country_mapping:\n",
    "            df.at[index, 'Country.of.Origin'] = farm_country_mapping[farm_name]\n",
    "    return df\n",
    "\n",
    "# 후처리 적용\n",
    "validation = postprocess_country(test, farm_country_mapping)\n",
    "\n",
    "\n",
    "### Lot.Number\n",
    "import re\n",
    "\n",
    "def map_country_of_origin_postprocessing(value):\n",
    "    if re.match(r'^\\d+/\\d+[A-Za-z]$', str(value)) or str(value).startswith('431') or re.match(r'^\\d{3}/\\d{2}$', str(value)) or 'Lot' in str(value):\n",
    "        return 0\n",
    "    elif re.match(r'^3-\\d{2}-\\d{4}$', str(value)) or re.match(r'^\\d{2}-\\d{4}$', str(value)):\n",
    "        return 1\n",
    "    elif re.match(r'^11/\\d+/\\d+$', str(value)) or re.match(r'^11-\\d+-\\d+$', str(value)):\n",
    "        return 2\n",
    "\n",
    "def check_country_of_origin(value):\n",
    "    # Combine all the conditions into a single regular expression pattern\n",
    "    pattern = (\n",
    "        r'^\\d+/\\d+[A-Za-z]$|^431|^(\\d{3}/\\d{2})|Lot'  # Condition 1\n",
    "        r'|^3-\\d{2}-\\d{4}$|^\\d{2}-\\d{4}$'  # Condition 2\n",
    "        r'|^11/\\d+/\\d+$|^11-\\d+-\\d+$'  # Condition 3\n",
    "    )\n",
    "    \n",
    "    # Check if the value matches any of the conditions\n",
    "    if re.match(pattern, str(value)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# 후처리 함수 정의\n",
    "def postprocess_country_of_origin(df):\n",
    "    for index, row in df.iterrows():\n",
    "        if check_country_of_origin(row['Lot.Number']):\n",
    "            df.at[index, 'Country.of.Origin'] = map_country_of_origin_postprocessing(row['Lot.Number'])\n",
    "    return df\n",
    "\n",
    "# test_data에 후처리 적용\n",
    "test = postprocess_country_of_origin(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 2, 0, 3, 2, 1, 3, 3, 3, 0, 3, 1, 3, 1, 2, 1, 1, 2, 3, 2, 3,\n",
       "       2, 2, 0, 3, 2, 3, 3, 3, 3, 1, 2, 3, 3, 0, 3, 2, 0, 1, 1, 3, 0, 2,\n",
       "       0, 0, 2, 0, 1, 2, 0, 0, 3, 2, 1, 1, 1, 0, 2, 2, 0, 2, 0, 2, 1, 0,\n",
       "       3, 1, 0, 2, 3, 2, 0, 1, 0, 2, 2, 3, 0, 3, 3, 1, 3, 3, 0, 3, 1, 3,\n",
       "       1, 2, 3, 3, 3, 2, 3, 2, 0, 0, 1, 1, 2, 1, 3, 0, 1, 0, 2, 1, 0, 0,\n",
       "       1, 2, 1, 3, 0, 2, 1, 0, 1, 3, 0, 1, 3, 1, 3, 2, 2, 2, 1, 2, 3, 2,\n",
       "       3, 3, 1, 0, 3, 0, 1, 2, 1, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Country.of.Origin'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"/Users/mingu/Desktop/CODING/Kaggle Project 1/DATASET/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['Country.of.Origin'] = test['Country.of.Origin'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      3\n",
       "1      2\n",
       "2      2\n",
       "3      0\n",
       "4      3\n",
       "      ..\n",
       "142    1\n",
       "143    0\n",
       "144    1\n",
       "145    0\n",
       "146    1\n",
       "Name: Country.of.Origin, Length: 147, dtype: int64"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub['Country.of.Origin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('/Users/mingu/Desktop/CODING/Kaggle Project 1/SUBMISSION/baseline_oversampling.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
